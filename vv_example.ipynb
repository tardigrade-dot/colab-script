{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ee8a005e",
      "metadata": {
        "id": "ee8a005e",
        "outputId": "6daf7e98-58d5-432d-d7a7-26da20f02e25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VibeVoice'...\n",
            "remote: Enumerating objects: 639, done.\u001b[K\n",
            "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 639 (delta 205), reused 177 (delta 177), pack-reused 414 (from 1)\u001b[K\n",
            "Receiving objects: 100% (639/639), 39.07 MiB | 27.01 MiB/s, done.\n",
            "Resolving deltas: 100% (385/385), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vibevoice-community/VibeVoice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rpgTW7Tr6A96",
        "outputId": "6dfc52cf-436f-4e33-be9e-e831a2f7432b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rpgTW7Tr6A96",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip --quiet install --system -e /content/VibeVoice"
      ],
      "metadata": {
        "id": "vV-pBHBZ6BhC"
      },
      "id": "vV-pBHBZ6BhC",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install wetext"
      ],
      "metadata": {
        "id": "oOGyJxm8AfO9",
        "outputId": "bd3f0259-0d7d-448c-ebbb-3dd25493e594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oOGyJxm8AfO9",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m6 packages\u001b[0m \u001b[2min 217ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m6 packages\u001b[0m \u001b[2min 155ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1manyascii\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcontractions\u001b[0m\u001b[2m==0.1.73\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkaldifst\u001b[0m\u001b[2m==1.7.17\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyahocorasick\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtextsearch\u001b[0m\u001b[2m==0.0.24\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwetext\u001b[0m\u001b[2m==0.1.2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import types\n",
        "from vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference\n",
        "from vibevoice.processor.vibevoice_processor import VibeVoiceProcessor\n",
        "from transformers.utils import logging\n",
        "from wetext import Normalizer\n",
        "from typing import Dict\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import re\n",
        "import os\n",
        "\n",
        "logging.set_verbosity_info()\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "sentence_splitter = [\"！\", \"；\", \"？\", \"～\", \"?\", \"!\", \"：\", \"～\", \"…\", \"……\", \"。\"]\n",
        "char_rep_map = {\n",
        "    \"——\":\".\", \"：\": \",\",\"；\": \",\",\";\": \",\",\"，\": \",\",\"。\": \".\",\"！\": \"!\",\"？\": \"?\",\"·\": \"-\",\n",
        "    \"、\": \",\",\"...\": \"…\",\",,,\": \"…\",\"，，，\": \"…\",\"……\": \"…\",\"“\": \"'\",\"”\": \"'\",\n",
        "    '\"': \"'\",\"‘\": \"'\",\"’\": \"'\",\"（\": \"'\",\"）\": \"'\",\"(\": \"'\",\")\": \"'\",\n",
        "    \"《\": \"'\",\"》\": \"'\",\"【\": \"'\",\"】\": \"'\",\"[\": \"'\",\"]\": \"'\",\"—\": \"-\",\n",
        "    \"～\": \"-\",\"~\": \"-\",\"「\": \"'\",\"」\": \"'\",\":\": \",\",\n",
        "    \"〇\": \"零\",\"○\": \"零\",\"卐\":\"万\"\n",
        "}\n",
        "\n",
        "def replace_chars(full_script, char_rep_map):\n",
        "    result = ''\n",
        "    for char in full_script:\n",
        "        result += char_rep_map.get(char, char)\n",
        "    return result\n",
        "\n",
        "class BookAudioGenerator:\n",
        "    def __init__(self, tts_model, device) -> None:\n",
        "        self.processor = VibeVoiceProcessor.from_pretrained(\n",
        "                tts_model,\n",
        "                device=device\n",
        "            )\n",
        "        model = VibeVoiceForConditionalGenerationInference.from_pretrained(\n",
        "                tts_model,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=device,)\n",
        "        model.eval()\n",
        "        model.set_ddpm_inference_steps(num_steps=10)\n",
        "        self.model=model\n",
        "\n",
        "        self.default_prefix=\"Speaker 1:\"\n",
        "        self.default_speaker = \"旁白\"\n",
        "        self.max_length_times = 3\n",
        "        self.normalizer = Normalizer(lang=\"zh\", operator=\"tn\", remove_erhua=True, traditional_to_simple=False)\n",
        "\n",
        "    def batch_process(self, i_file, batch_size, process_size):\n",
        "        def _read_file():\n",
        "            _lines = []\n",
        "            with open(i_file, 'r', encoding='utf-8') as f: # 按行分割, 保证在一个段落的内容都放在一个输入中, 避免从中间拆分\n",
        "                _lines = f.read().splitlines()\n",
        "            if not _lines:\n",
        "                raise Exception(f'not content in {i_file}')\n",
        "\n",
        "            results = []  # 存放一批(batch_size)的结果\n",
        "            current_lines = []  # 当前正在积累的块\n",
        "            current_length = 0  # 当前块的字符总长度\n",
        "\n",
        "            for _line in _lines:\n",
        "                if _line:\n",
        "                    current_lines.append(_line)\n",
        "                    current_length += len(_line)\n",
        "                    if current_length >= process_size:\n",
        "                        results.append(current_lines)\n",
        "                        current_lines = []\n",
        "                        current_length = 0\n",
        "                        if len(results) == batch_size:\n",
        "                            yield results\n",
        "                            results = []\n",
        "            if current_lines:\n",
        "                results.append(current_lines)\n",
        "            if results:\n",
        "                yield results\n",
        "\n",
        "        batch_index = 0\n",
        "        for batch in _read_file():\n",
        "            processed_batch = []\n",
        "            for sub_list in batch:\n",
        "                processed_sub_list = []\n",
        "\n",
        "                for item in sub_list:\n",
        "                    stripped_item = item.strip()\n",
        "                    if stripped_item:\n",
        "                        stripped_item.split() # 将一个段落拆分,避免一句话中内容太多,导致输出语音语速变快\n",
        "                        processed_sub_list.extend(self.split_sentence(stripped_item))\n",
        "\n",
        "                processed_batch.append(processed_sub_list)\n",
        "            yield processed_batch, batch_index\n",
        "            batch_index += 1\n",
        "\n",
        "    def split_sentence(self, sentence):\n",
        "        splitters = \"\".join(sentence_splitter)\n",
        "        escaped_splitters = re.escape(splitters)\n",
        "        pattern = r'([' + escaped_splitters + r'])\\s*'\n",
        "        parts = re.split(pattern, sentence)\n",
        "        sentences = []\n",
        "        current_sentence = \"\"\n",
        "        for part in parts:\n",
        "            if part is None or not part.strip():\n",
        "                continue\n",
        "            current_sentence += part\n",
        "            if part in sentence_splitter:\n",
        "                sentences.append(current_sentence.strip())\n",
        "                current_sentence = \"\"\n",
        "        if current_sentence.strip():\n",
        "            sentences.append(current_sentence.strip())\n",
        "        return sentences\n",
        "\n",
        "    def _tts_generate(self, to_tts_batch, voice_sample):\n",
        "        inputs = self.processor(\n",
        "            text=to_tts_batch,\n",
        "            voice_samples=voice_sample,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=None,\n",
        "            cfg_scale=1.3,\n",
        "            tokenizer=self.processor.tokenizer,\n",
        "            # generation_config={'do_sample': True, 'temperature': 0.99, 'top_p': 0.99, 'top_k': 3},\n",
        "            generation_config={'do_sample': False},\n",
        "            verbose=True,\n",
        "            max_length_times=self.max_length_times, #default 2\n",
        "        )\n",
        "        return outputs\n",
        "\n",
        "    def txt_normlize(self, txt):\n",
        "        return self.normalizer.normalize(txt)\n",
        "\n",
        "    def tts_txt_preprocess(self, txt):\n",
        "\n",
        "        chinese_pattern = r\"（.*?）\"\n",
        "        english_pattern = r\"\\([^)]*?\\)\"\n",
        "\n",
        "        combined_pattern = f\"{chinese_pattern}|{english_pattern}\"\n",
        "        _txt = re.sub(combined_pattern, \"\", txt)\n",
        "        _txt = self.default_prefix + replace_chars(_txt, char_rep_map) #规范化, 替换中文符号, 根据vibevoice文档, 建议使用英语标点符号\n",
        "        return _txt\n",
        "\n",
        "    def gererator_speech_with_default_voice(\n",
        "            self,\n",
        "            chunk,\n",
        "            batch_index,\n",
        "            single_speaker,\n",
        "            output_dir\n",
        "            ):\n",
        "        # txt_normlize 通过opencc将繁体转换成了简体.txt可以直接保存简体部分\n",
        "        to_tts_batch = [\n",
        "            [\n",
        "                self.txt_normlize(item) for item in s_batch\n",
        "            ]\n",
        "            for s_batch in chunk\n",
        "            ]\n",
        "\n",
        "        _tts_text = [\n",
        "            \"\\n\".join([\n",
        "                self.tts_txt_preprocess(item) for item in s_batch\n",
        "            ])\n",
        "            for s_batch in to_tts_batch\n",
        "            ]\n",
        "\n",
        "        output_stem = output_path_wav = f\"{output_dir}/{project_name}-{batch_index}\"\n",
        "        output_path_wav = f\"{output_stem}_0.wav\"\n",
        "        if os.path.exists(output_path_wav):\n",
        "            logger.warning(f'⚠️ file {output_path_wav} exists, so batch will not process.')\n",
        "            return\n",
        "\n",
        "        outputs = self._tts_generate(_tts_text, [[single_speaker]] * len(chunk))\n",
        "        for check in outputs.reach_max_step_sample.tolist():\n",
        "            if check:\n",
        "                logger.warning(f'⚠️  reach max length, audio may cut up, you may increase [max_length_times] and current is [{self.max_length_times}]')\n",
        "\n",
        "        for _index, (output_speech, txt) in enumerate(zip(outputs.speech_outputs, chunk)): #\n",
        "\n",
        "            output_path_wav = f\"{output_stem}_{_index}.wav\"\n",
        "            output_path_txt = f\"{output_stem}_{_index}.txt\"\n",
        "\n",
        "            output_path = Path(output_path_txt)\n",
        "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            self.processor.save_audio(\n",
        "                output_speech,\n",
        "                output_path=output_path_wav,\n",
        "            )\n",
        "            output_path.write_text(\"\\n\".join(txt), encoding='utf-8')\n",
        "            logger.info(f'finish process ouput file : {output_path_wav} \\n {output_path_txt}')\n",
        "\n",
        "    def generate(self, to_tts_file, output_dir, single_speaker, batch_size = 4, process_size = 9000):\n",
        "        for _b, _i in self.batch_process(to_tts_file, batch_size, process_size):\n",
        "            self.gererator_speech_with_default_voice(_b, _i, single_speaker, output_dir)\n",
        "\n",
        "    def generate_single_dialog(self, to_tts_file, txt_speeker, speeker_voice):\n",
        "\n",
        "        with open(to_tts_file, 'r', encoding='utf-8') as f:\n",
        "            _lines = f.read().splitlines()\n",
        "        output_path_wav = Path(to_tts_file).with_suffix(\".wav\")\n",
        "\n",
        "        speeker_voice_x = [f\"Speaker {i+1}\" for i, speaker in enumerate(txt_speeker)]\n",
        "        speaker_map: Dict[str, str] = dict(zip(txt_speeker, speeker_voice_x))\n",
        "\n",
        "        SPEAKER_PATTERN = re.compile(r'^([^:]+):')\n",
        "\n",
        "        to_tts_batch = []\n",
        "        pre_speaker = self.default_speaker\n",
        "        for item in _lines:\n",
        "            if item:\n",
        "                match = SPEAKER_PATTERN.match(item)\n",
        "                if match:\n",
        "                    speaker_name = match.group(1).strip()\n",
        "                    selected_prefix = speaker_map.get(speaker_name, self.default_speaker[0])\n",
        "                    item_content = item[match.end():].strip() # 提取冒号后的内容\n",
        "                    new_line = selected_prefix + \": \" + item_content\n",
        "                    pre_speaker = speaker_name\n",
        "                else:\n",
        "                    speaker_name = pre_speaker\n",
        "                    new_line = speaker_map.get(speaker_name, self.default_speaker[0]) + \": \" + item\n",
        "                to_tts_batch.append(new_line)\n",
        "\n",
        "        to_tts_batch = [\"\\n\".join(to_tts_batch)]\n",
        "\n",
        "        outputs = self._tts_generate(to_tts_batch, speeker_voice)\n",
        "        self.processor.save_audio(\n",
        "            outputs.speech_outputs[0],\n",
        "            output_path=output_path_wav,\n",
        "        )\n",
        "\n",
        "    def generate_single(self, to_tts_file, voice_samples):\n",
        "        with open(to_tts_file, 'r', encoding='utf-8') as f:\n",
        "            _lines = f.read().splitlines()\n",
        "\n",
        "        output_path_wav = Path(to_tts_file).with_suffix(\".wav\")\n",
        "        to_tts_txt = [self.default_prefix + item for item in _lines]\n",
        "        to_tts_txt = \"\\n\".join(to_tts_txt)\n",
        "        to_tts_txt = self.txt_normlize(to_tts_txt)\n",
        "        to_tts_txt = self.tts_txt_preprocess(to_tts_txt)\n",
        "        outputs = self._tts_generate(to_tts_txt, [voice_samples])\n",
        "        self.processor.save_audio(\n",
        "            outputs.speech_outputs[0],\n",
        "            output_path=output_path_wav,\n",
        "        )\n",
        "\n",
        "env_type = \"colab\" # colab modelscope local\n",
        "\n",
        "env_config = {\n",
        "    \"local\":{\n",
        "        \"drive_dir\" : \"/Volumes/sw/MyDrive\",\n",
        "        # \"model_name\": \"/Volumes/sw/pretrained_models/VibeVoice-1.5B\",\n",
        "        \"model_name\": \"/Volumes/sw/hf_models/VibeVoice-1.5B-ft\",\n",
        "        \"device\": \"mps\"\n",
        "    },\n",
        "    \"modelscope\":{\n",
        "        \"drive_dir\": \"/mnt/workspace\",\n",
        "        # model_name = \"/mnt/workspace/pretrained_models/VibeVoice-1.5B\"\n",
        "        \"model_name\": \"/mnt/workspace/pretrained_models/VibeVoice-1.5B-ft\",\n",
        "        \"device\": \"cuda\"\n",
        "    },\n",
        "    \"colab\":{\n",
        "        \"drive_dir\": \"/content/drive/MyDrive\",\n",
        "        \"model_name\": \"tardigrade-doc/VibeVoice-1.5B-ft\",\n",
        "        # \"model_name\": \"microsoft/VibeVoice-1.5B\",\n",
        "        \"device\": \"cuda\"\n",
        "    }\n",
        "}\n",
        "if env_type not in env_config:\n",
        "    raise Exception(f\"not supported env {env_type}\")\n",
        "config_dict = env_config[env_type]\n",
        "\n",
        "config = types.SimpleNamespace(**config_dict)\n",
        "\n",
        "drive_dir = config.drive_dir\n",
        "model_name = config.model_name\n",
        "device = config.device\n",
        "\n",
        "input_file = f\"{drive_dir}/data_src/tianchaoyaoyuan2.txt\"\n",
        "speaker_phi0 = f\"{drive_dir}/data_src/qinsheng.wav\"\n",
        "\n",
        "input_file_path = Path(input_file)\n",
        "project_name = input_file_path.stem\n",
        "\n",
        "output_dir = f\"{drive_dir}/{project_name}\"\n",
        "bookAudioGen = BookAudioGenerator(\n",
        "    model_name,\n",
        "    device)\n",
        "\n",
        "bookAudioGen.generate(input_file, output_dir, speaker_phi0, 4, 8000)\n",
        "# bookAudioGen.generate_single(\"/content/drive/MyDrive/fubaiyufanfu/fubaiyufanfu-1_0.txt\", speaker_phi0)\n",
        "# 针对某个已经经过上述批量处理后,某个txt对应的wav存在问题的重新生成.\n",
        "# bookAudioGen.generate_single(\"/Volumes/sw/MyDrive/zhengzhi1/output/zhengzhi1-4_2.txt\", [speaker_phi0])\n",
        "\n",
        "# bookAudioGen.generate_single(\"/Volumes/sw/tmp/zhengzhi1-5_4.txt\", [speaker_phi0])\n",
        "\n",
        "# bookAudioGen.generate_single_dialog(\n",
        "#     \"/Users/larry/github.com/tardigrade-dot/colab-script/data_src/sugeladizhisi_part1.txt\",\n",
        "#     [\"旁白\", \"欧\", \"苏\"],\n",
        "#     [f\"{drive_dir}/data_src/youyi.wav\", f\"{drive_dir}/data_src/sample_zhongdong.wav\", f\"{drive_dir}/data_src/gdg_voice_06.wav\"])\n"
      ],
      "metadata": {
        "id": "N6nKYjf36SS-",
        "outputId": "a1a775a4-9c3e-4add-8b5d-4206b9716291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "79090dd1bc0d4774802143d21ae48e15",
            "51e64ccc2c344aa8bd216596bc0f75e0",
            "5ec9b48807c74248b2637f65456afe39",
            "b7ed6eb99fd04e8f8b0af0160a46e509",
            "d98b229b089e48ccac544c0a3e119291",
            "10b8a6184ae942e7b16aa7b1c98116e1",
            "54846577b4da4f1b839f4a3ebbfa327c",
            "72add29f544c4745b94aebfb8ad6a8f5",
            "95cd43e79093473c8138ba1a87278dc4",
            "b866be4296e244c2b25a0c64eee8891c",
            "1000b3117e5841b1a9f034a4b4610858",
            "19ee65f15eab48c2947ec5dd962930cd"
          ]
        }
      },
      "id": "N6nKYjf36SS-",
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:vibevoice.modular.modular_vibevoice_tokenizer:APEX FusedRMSNorm not available, using native implementation\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79090dd1bc0d4774802143d21ae48e15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/351 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51e64ccc2c344aa8bd216596bc0f75e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ec9b48807c74248b2637f65456afe39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7ed6eb99fd04e8f8b0af0160a46e509",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d98b229b089e48ccac544c0a3e119291",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Qwen2Tokenizer'. \n",
            "The class this function is called from is 'VibeVoiceTextTokenizerFast'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10b8a6184ae942e7b16aa7b1c98116e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-1.5B/snapshots/1904eae38036e9c780d28e27990c27748984eafe/config.json\n",
            "Model config VibeVoiceConfig {\n",
            "  \"acoustic_tokenizer_config\": {\n",
            "    \"causal\": true,\n",
            "    \"channels\": 1,\n",
            "    \"conv_bias\": true,\n",
            "    \"conv_norm\": \"none\",\n",
            "    \"corpus_normalize\": 0.0,\n",
            "    \"decoder_depths\": null,\n",
            "    \"decoder_n_filters\": 32,\n",
            "    \"decoder_ratios\": [\n",
            "      8,\n",
            "      5,\n",
            "      5,\n",
            "      4,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    \"disable_last_norm\": true,\n",
            "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
            "    \"encoder_n_filters\": 32,\n",
            "    \"encoder_ratios\": [\n",
            "      8,\n",
            "      5,\n",
            "      5,\n",
            "      4,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    \"fix_std\": 0.5,\n",
            "    \"layer_scale_init_value\": 1e-06,\n",
            "    \"layernorm\": \"RMSNorm\",\n",
            "    \"layernorm_elementwise_affine\": true,\n",
            "    \"layernorm_eps\": 1e-05,\n",
            "    \"mixer_layer\": \"depthwise_conv\",\n",
            "    \"model_type\": \"vibevoice_acoustic_tokenizer\",\n",
            "    \"pad_mode\": \"constant\",\n",
            "    \"std_dist_type\": \"gaussian\",\n",
            "    \"vae_dim\": 64,\n",
            "    \"weight_init_value\": 0.01\n",
            "  },\n",
            "  \"acoustic_vae_dim\": 64,\n",
            "  \"architectures\": [\n",
            "    \"VibeVoiceForConditionalGeneration\"\n",
            "  ],\n",
            "  \"decoder_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 1536,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 8960,\n",
            "    \"max_position_embeddings\": 65536,\n",
            "    \"max_window_layers\": 28,\n",
            "    \"model_type\": \"qwen2\",\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_hidden_layers\": 28,\n",
            "    \"num_key_value_heads\": 2,\n",
            "    \"rms_norm_eps\": 1e-06,\n",
            "    \"rope_scaling\": null,\n",
            "    \"rope_theta\": 1000000.0,\n",
            "    \"sliding_window\": null,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"torch_dtype\": \"bfloat16\",\n",
            "    \"use_cache\": true,\n",
            "    \"use_sliding_window\": false,\n",
            "    \"vocab_size\": 151936\n",
            "  },\n",
            "  \"diffusion_head_config\": {\n",
            "    \"ddpm_batch_mul\": 4,\n",
            "    \"ddpm_beta_schedule\": \"cosine\",\n",
            "    \"ddpm_num_inference_steps\": 20,\n",
            "    \"ddpm_num_steps\": 1000,\n",
            "    \"diffusion_type\": \"ddpm\",\n",
            "    \"head_ffn_ratio\": 3.0,\n",
            "    \"head_layers\": 4,\n",
            "    \"hidden_size\": 1536,\n",
            "    \"latent_size\": 64,\n",
            "    \"model_type\": \"vibevoice_diffusion_head\",\n",
            "    \"prediction_type\": \"v_prediction\",\n",
            "    \"rms_norm_eps\": 1e-05,\n",
            "    \"speech_vae_dim\": 64\n",
            "  },\n",
            "  \"model_type\": \"vibevoice\",\n",
            "  \"semantic_tokenizer_config\": {\n",
            "    \"causal\": true,\n",
            "    \"channels\": 1,\n",
            "    \"conv_bias\": true,\n",
            "    \"conv_norm\": \"none\",\n",
            "    \"corpus_normalize\": 0.0,\n",
            "    \"disable_last_norm\": true,\n",
            "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
            "    \"encoder_n_filters\": 32,\n",
            "    \"encoder_ratios\": [\n",
            "      8,\n",
            "      5,\n",
            "      5,\n",
            "      4,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    \"fix_std\": 0,\n",
            "    \"layer_scale_init_value\": 1e-06,\n",
            "    \"layernorm\": \"RMSNorm\",\n",
            "    \"layernorm_elementwise_affine\": true,\n",
            "    \"layernorm_eps\": 1e-05,\n",
            "    \"mixer_layer\": \"depthwise_conv\",\n",
            "    \"model_type\": \"vibevoice_semantic_tokenizer\",\n",
            "    \"pad_mode\": \"constant\",\n",
            "    \"std_dist_type\": \"none\",\n",
            "    \"vae_dim\": 128,\n",
            "    \"weight_init_value\": 0.01\n",
            "  },\n",
            "  \"semantic_vae_dim\": 128,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54846577b4da4f1b839f4a3ebbfa327c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-1.5B/snapshots/1904eae38036e9c780d28e27990c27748984eafe/model.safetensors.index.json\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72add29f544c4745b94aebfb8ad6a8f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95cd43e79093473c8138ba1a87278dc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b866be4296e244c2b25a0c64eee8891c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1000b3117e5841b1a9f034a4b4610858",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Instantiating VibeVoiceForConditionalGenerationInference model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {}\n",
            "\n",
            "Instantiating Qwen2Model model under default dtype torch.bfloat16.\n",
            "Instantiating VibeVoiceAcousticTokenizerModel model under default dtype torch.bfloat16.\n",
            "Instantiating VibeVoiceSemanticTokenizerModel model under default dtype torch.bfloat16.\n",
            "Instantiating VibeVoiceDiffusionHead model under default dtype torch.bfloat16.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19ee65f15eab48c2947ec5dd962930cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing VibeVoiceForConditionalGenerationInference.\n",
            "\n",
            "All the weights of VibeVoiceForConditionalGenerationInference were initialized from the model checkpoint at microsoft/VibeVoice-1.5B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use VibeVoiceForConditionalGenerationInference for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "WARNING:__main__:⚠️ file /content/drive/MyDrive/tianchaoyaoyuan1/tianchaoyaoyuan1-0_0.wav exists, so batch will not process.\n",
            "WARNING:__main__:⚠️ file /content/drive/MyDrive/tianchaoyaoyuan1/tianchaoyaoyuan1-1_0.wav exists, so batch will not process.\n",
            "WARNING:__main__:⚠️ file /content/drive/MyDrive/tianchaoyaoyuan1/tianchaoyaoyuan1-2_0.wav exists, so batch will not process.\n",
            "Generating (active: 2/2):  68%|██████▊   | 13156/19425 [43:19<25:40,  4.07it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [0] reached EOS token at step 13157.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 1/2):  75%|███████▍  | 14495/19425 [48:51<17:10,  4.78it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [1] reached EOS token at step 14496.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 2/2):  65%|██████▌   | 13501/20673 [45:05<30:15,  3.95it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [1] reached EOS token at step 13502.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 1/2):  71%|███████▏  | 14775/20673 [50:21<17:01,  5.78it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [0] reached EOS token at step 14776.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 2/2):  72%|███████▏  | 14374/19899 [48:26<23:35,  3.90it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [0] reached EOS token at step 14375.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 1/2):  78%|███████▊  | 15606/19899 [53:45<15:33,  4.60it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [1] reached EOS token at step 15607.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 2/2):  41%|████      | 7477/18435 [22:12<36:33,  5.00it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [1] reached EOS token at step 7478.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 1/2):  60%|██████    | 11139/18435 [34:56<27:36,  4.40it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [0] reached EOS token at step 11141.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 2/2):  61%|██████▏   | 12394/20205 [40:34<31:25,  4.14it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [0] reached EOS token at step 12395.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 1/2):  69%|██████▊   | 13875/20205 [46:30<26:25,  3.99it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [1] reached EOS token at step 13877.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 2/2):  69%|██████▉   | 14867/21588 [51:22<29:43,  3.77it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [1] reached EOS token at step 14868.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 1/2):  74%|███████▎  | 15871/21588 [55:46<21:10,  4.50it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [0] reached EOS token at step 15872.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 2/2):  75%|███████▍  | 15341/20532 [53:11<22:53,  3.78it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [0] reached EOS token at step 15342.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (active: 1/2):  77%|███████▋  | 15721/20532 [54:51<17:35,  4.56it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples [1] reached EOS token at step 15722.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 2/2):  71%|███████   | 14553/20508 [49:43<25:47,  3.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [1] reached EOS token at step 14554.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 1/2):  71%|███████   | 14611/20508 [49:58<20:47,  4.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [0] reached EOS token at step 14612.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 2/2):  80%|███████▉  | 15269/19170 [51:58<17:04,  3.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [1] reached EOS token at step 15270.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 1/2):  82%|████████▏ | 15804/19170 [54:16<12:04,  4.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [0] reached EOS token at step 15805.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 2/2):  66%|██████▋   | 13855/20895 [46:21<29:43,  3.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [1] reached EOS token at step 13856.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 1/2):  69%|██████▉   | 14417/20895 [48:41<22:38,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [0] reached EOS token at step 14418.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 2/2):  68%|██████▊   | 13583/19953 [45:19<27:00,  3.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [1] reached EOS token at step 13584.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 1/2):  79%|███████▉  | 15753/19953 [54:39<15:22,  4.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [0] reached EOS token at step 15754.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 2/2):  72%|███████▏  | 14111/19734 [48:11<24:19,  3.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [1] reached EOS token at step 14112.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 1/2):  79%|███████▉  | 15612/19734 [54:38<15:08,  4.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [0] reached EOS token at step 15613.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 2/2):  73%|███████▎  | 14440/19674 [50:00<22:55,  3.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [1] reached EOS token at step 14441.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 1/2):  74%|███████▍  | 14513/19674 [50:19<18:28,  4.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [0] reached EOS token at step 14514.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 2/2):  32%|███▏      | 6139/19056 [18:40<42:28,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [1] reached EOS token at step 6140.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating (active: 1/2):  86%|████████▌ | 16297/19056 [57:35<10:17,  4.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples [0] reached EOS token at step 16298.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}